{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import hypergrad as hg\n",
    "\n",
    "TARGET_DEVICE = th.device('cuda') if th.cuda.is_available() else th.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for training a simple equilibrium network with an \"RNN-style\" dynamics on a subset of Mnist data.\n",
    "\n",
    "For more details refer to Section 4.2 of the paper \n",
    "_On the iteration complexity of hypergradient computations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# UTILS\n",
    "# --------------------------------------------\n",
    "\n",
    "def store(tensor):\n",
    "    if isinstance(tensor, list):\n",
    "        return [store(v) for v in tensor]\n",
    "    else:\n",
    "        return tensor.detach().to(th.device('cpu')).numpy()\n",
    "    \n",
    "\n",
    "def set_requires_grad(lst): [l.requires_grad_(True) for l in lst]\n",
    "\n",
    "\n",
    "def acc(preds, targets):\n",
    "    \"\"\"Computes the accuracy\"\"\"\n",
    "    return preds.argmax(dim=1).eq(targets).float().mean()\n",
    "\n",
    "    \n",
    "\n",
    "class NamedLists(list):\n",
    "    def __init__(self, lst, names) -> None:\n",
    "        super().__init__(lst)\n",
    "        assert len(lst) == len(names)\n",
    "        self.names = names\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if isinstance(i, str):\n",
    "            return self.__getattribute__(i)\n",
    "        else:\n",
    "            return super().__getitem__(i)\n",
    "\n",
    "\n",
    "class TVT(NamedLists):  # train val & test\n",
    "    def __init__(self, lst) -> None:\n",
    "        super().__init__(lst, ['train', 'val', 'test'])\n",
    "        self.train, self.val, self.test = lst\n",
    "\n",
    "\n",
    "class DT(NamedLists):  # data & targets\n",
    "    def __init__(self, lst) -> None:\n",
    "        super().__init__(lst, ['data', 'targets'])\n",
    "        self.data, self.targets = lst\n",
    "\n",
    "\n",
    "class LA(NamedLists):  # loss and accuracy\n",
    "    def __init__(self, lst):\n",
    "        super().__init__(lst, ['loss', 'acc'])\n",
    "        self.loss, self.acc = lst\n",
    "\n",
    "\n",
    "def load_mnist(seed=0, num_train=50000, num_valid=10000):\n",
    "    \"\"\"Load MNIST dataset with given number of training and validation examples\"\"\"\n",
    "    from torchvision import datasets\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    mnist_train = datasets.MNIST('../data', download=True, train=True)\n",
    "    train_indices = rnd.permutation(list(range(60000)))\n",
    "    dta, targets = mnist_train.data, mnist_train.targets\n",
    "\n",
    "    # print(train_indices)\n",
    "    tr_inds = train_indices[:num_train]\n",
    "    mnist_tr1 = DT([dta[tr_inds], targets[tr_inds]])\n",
    "\n",
    "    val_inds = train_indices[num_train:num_train + num_valid]\n",
    "    mnist_valid = DT([dta[val_inds], targets[val_inds]])\n",
    "\n",
    "    mnist_test = datasets.MNIST('../data', download=True, train=False)\n",
    "\n",
    "    def _process_dataset(dts):\n",
    "        dt, tgt = np.array(dts.data.numpy(), dtype=np.float32), dts.targets.numpy()\n",
    "        return DT([th.from_numpy(\n",
    "            np.reshape(dt / 255., (-1, 28 * 28))).to(TARGET_DEVICE),\n",
    "                   th.from_numpy(tgt).to(TARGET_DEVICE)])\n",
    "\n",
    "    return TVT([_process_dataset(dtt) for dtt in [mnist_tr1, mnist_valid, mnist_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sig = 0.01  # initialization \n",
    "dw = 200# dimensionality of the hidden state \n",
    "\n",
    "lr = 0.5\n",
    "\n",
    "\n",
    "th.manual_seed(0)\n",
    "data = load_mnist(0, num_train=5000, num_valid=5000)\n",
    "num_exp, dim_x = data.train.data.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_projection = True  \n",
    "\n",
    "T = K = 20  # number of iterations; T for forward iterations, K for backward;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose between \n",
    "# rm (reverse-mode iterative differentiation),\n",
    "# fp (fixed point implicit differentiation) and\n",
    "# cg (conjugate gradient implicit differentiation)\n",
    "\n",
    "hg_mode = 'rm'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that define the dynamics (see eq )\n",
    "def phi_simple_RNN_like(x):\n",
    "    def _phi(w, lmd):\n",
    "        A, B, c = lmd[:3]\n",
    "        ww = w[0]\n",
    "        return [th.tanh(ww @ A + x @ B + c)]\n",
    "\n",
    "    return _phi\n",
    "\n",
    "phi = phi_simple_RNN_like  # change this line for changing type of dynamics \n",
    "\n",
    "# obtain one dynamics per set (training, validation and test) which is a callable\n",
    "PHIs = TVT([phi(dt.data) for dt in data])\n",
    "\n",
    "\n",
    "def matrix_projection_on_spectral_ball(a, radius=0.999, project=True):\n",
    "    A = a.detach()\n",
    "    if A.is_cuda: A = A.cpu()\n",
    "    A = A.numpy()\n",
    "    U, S, V = np.linalg.svd(A)\n",
    "    if project:\n",
    "        S1 = np.minimum(S, radius)\n",
    "        a = U @ np.diag(S1) @ V\n",
    "    else:\n",
    "        a = A\n",
    "    return th.from_numpy(a).type(th.FloatTensor).to(TARGET_DEVICE).requires_grad_(True), S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial state\n",
    "w0s = TVT([th.zeros(d.data.shape[0], dw, device=TARGET_DEVICE) for d in data])\n",
    "\n",
    "if hg == 'rm': set_requires_grad(w0s)  # necessary only for reverse-mode ITD\n",
    "\n",
    "# define model's parameters\n",
    "lmbd = [\n",
    "    i_sig * th.randn(dw, dw, device=TARGET_DEVICE),\n",
    "    i_sig * th.randn(dim_x, dw, device=TARGET_DEVICE),\n",
    "    i_sig * th.randn(dw, device=TARGET_DEVICE),\n",
    "    i_sig * th.randn(dw, 10, device=TARGET_DEVICE),\n",
    "    th.zeros(10, device=TARGET_DEVICE)\n",
    "]\n",
    "set_requires_grad(lmbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the linear model for computing the output\n",
    "def out_lin_mod(ww, lmd):\n",
    "    D, e1 = lmd[-2], lmd[-1]\n",
    "    return ww[0] @ D + e1\n",
    "\n",
    "# cross entropy losses \n",
    "def build_loss(tgts):\n",
    "    def loss(ww, lmd):\n",
    "        ce = th.nn.CrossEntropyLoss()\n",
    "        outputs = out_lin_mod(ww, lmd)\n",
    "        return th.mean(ce(outputs, tgts))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# obtain one loss per dataset (note: the losses remain callable as well as the dynamics!).   \n",
    "lss = TVT([build_loss(dt.targets) for dt in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "def fw(w0, the_phi):\n",
    "    def _f():\n",
    "        _vals = [[w0]]\n",
    "        for k in range(T):\n",
    "            _vals.append(the_phi(_vals[-1], lmbd))\n",
    "        return _vals\n",
    "\n",
    "    return _f\n",
    "\n",
    "# one per dataset\n",
    "fws = TVT([fw(ww, ph) for ww, ph in zip(w0s, PHIs)])\n",
    "\n",
    "\n",
    "def stat_after_fw(fww, which):\n",
    "    def _f():\n",
    "        _vls = fww()\n",
    "        return which(_vls[-1], lmbd)\n",
    "    return _f\n",
    "\n",
    "def accuracy(tgts):\n",
    "    def _f(w, lmd):\n",
    "        return acc(out_lin_mod(w, lmd), tgts)\n",
    "    return _f\n",
    "\n",
    "# obtrain callables for loss and accuracy for each training set (after executing the model's dyanmics)\n",
    "lss_acs_after_fw = TVT([\n",
    "    LA([stat_after_fw(fww, lss), stat_after_fw(fww, accuracy(dt.targets))])\n",
    "    for fww, lss, dt in zip(fws, lss, data)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "opt = th.optim.SGD(lmbd, lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at iteration 0: 0.5084\n",
      "Validation accuracy at iteration 20: 0.5118\n",
      "Validation accuracy at iteration 40: 0.111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cfeea0db1828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mhg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCG_normaleq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPHIs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhg_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rm'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mhg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse_unroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} not available!'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhg_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\hypergrad\\hypergradients.py\u001b[0m in \u001b[0;36mreverse_unroll\u001b[1;34m(params, hparams, outer_loss, set_grad)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreverse_unroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouter_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mo_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mouter_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mset_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mupdate_tensor_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    156\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[0;32m    157\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         inputs, allow_unused)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training!\n",
    "\n",
    "for t in range(1000):\n",
    "    vals = fws.train()  # compute the w_T\n",
    "\n",
    "    # compute the hypergradient (with different methods)\n",
    "    if hg_mode == 'fp':\n",
    "        hg.fixed_point(vals[-1], lmbd, K, PHIs.train, lss.train)\n",
    "    elif hg_mode == 'cg':\n",
    "        hg.CG_normaleq(vals[-1], lmbd, K, PHIs.train, lss.train)\n",
    "    elif hg_mode == 'rm':\n",
    "        hg.reverse_unroll(vals[-1], lmbd, lss.train)\n",
    "    else:\n",
    "        raise NotImplementedError('{} not available!'.format(hg_mode))\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    try:  # perform projection\n",
    "        A_proj, svl = matrix_projection_on_spectral_ball(lmbd[0], project=do_projection)\n",
    "        lmbd[0].data = A_proj.data\n",
    "    except (ValueError, np.linalg.LinAlgError) as e:\n",
    "        print('there were nans most probably: aborting all')\n",
    "        break\n",
    "\n",
    "    if t % 20 == 0:\n",
    "        valid_acc = store(lss_acs_after_fw.val.acc())\n",
    "        hgs = store([l.grad for l in lmbd])\n",
    "\n",
    "        print('Validation accuracy at iteration {}:'.format(t), valid_acc)  # update early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
